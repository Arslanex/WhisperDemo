{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T20:54:45.745019Z",
     "start_time": "2024-11-22T20:54:44.962239Z"
    }
   },
   "source": "from whisper_transcriber import WhisperTranscriber, TranscriptionConfig",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:56:08.498960Z",
     "start_time": "2024-11-22T20:56:08.495509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = TranscriptionConfig(\n",
    "    # Model Seçimi\n",
    "    model_name='medium',         # Seçenekler: tiny, base, small, medium, large\n",
    "    task='transcribe',      # 'transcribe' veya 'translate'\n",
    "\n",
    "    # Kalite Kontrol\n",
    "    temperature=[0.0, 0.2],  # Birden fazla örnekleme sıcaklığı\n",
    "    compression_ratio_threshold=2.4,\n",
    "    logprob_threshold=-1.0,\n",
    "    no_speech_threshold=0.6,\n",
    "\n",
    "    # Segment Filtreleme\n",
    "    max_segment_length=50,  # Segment başına maksimum karakter\n",
    "    min_segment_length=10,  # Segment başına minimum karakter\n",
    "\n",
    "    # Çıktı Formatlandırma\n",
    "    word_timestamps=True,   # Kelime düzeyinde zamanlama\n",
    "    verbose=True            # Detaylı log kaydı\n",
    ")"
   ],
   "id": "f87cb45be5f606d7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:56:58.102025Z",
     "start_time": "2024-11-22T20:56:49.673213Z"
    }
   },
   "cell_type": "code",
   "source": "transcriber = WhisperTranscriber(config)",
   "id": "b3f0e557bc3cdfe0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enesa/PycharmProjects/WhisperDemo/.venv/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:00:44.311891Z",
     "start_time": "2024-11-22T20:57:34.574350Z"
    }
   },
   "cell_type": "code",
   "source": "result = transcriber.transcribe('demo.mp3')",
   "id": "32bacc7d5bd14f90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enesa/PycharmProjects/WhisperDemo/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:03.660]  Imagine you're a movie star with both an assistant and an agent.\n",
      "[00:03.840 --> 00:05.900]  You know, I was chatting to both of mine just yesterday.\n",
      "[00:06.640 --> 00:07.360]  Sure you were.\n",
      "[00:08.680 --> 00:11.140]  Now, assistants, they help with scheduling,\n",
      "[00:11.800 --> 00:15.280]  and agents are more proactive, seeking opportunities for you.\n",
      "[00:15.820 --> 00:17.820]  Artificial intelligence works much the same way.\n",
      "[00:18.140 --> 00:20.700]  You have AI assistants and AI agents.\n",
      "[00:21.160 --> 00:23.320]  And in this video, we are going to explore\n",
      "[00:23.320 --> 00:26.880]  how both of these types of AI are shaping the future of work.\n",
      "[00:27.120 --> 00:29.420]  Right. And at a fundamental level,\n",
      "[00:29.420 --> 00:31.600]  I think we can say the main difference is that AI assistants,\n",
      "[00:31.820 --> 00:33.200]  they're reactive, right?\n",
      "[00:33.240 --> 00:36.360]  They're waiting for commands like a prompt from the user,\n",
      "[00:36.580 --> 00:38.680]  while AI agents are proactive.\n",
      "[00:38.800 --> 00:40.580]  They're acting autonomously to achieve a goal.\n",
      "[00:41.260 --> 00:42.420]  So, when you say we get into it.\n",
      "[00:42.540 --> 00:43.480]  Let's do it.\n",
      "[00:43.840 --> 00:45.840]  So let's start with AI assistants.\n",
      "[00:47.100 --> 00:50.200]  These helpful little apps understand natural language,\n",
      "[00:50.240 --> 00:53.920]  and they're great for doing things like organizing information\n",
      "[00:53.920 --> 00:55.720]  or responding to customer queries.\n",
      "[00:55.720 --> 01:00.220]  Examples that we know are Siri, Alexa, or chat GPT.\n",
      "[01:00.520 --> 01:01.840]  Right, and most AI assistants,\n",
      "[01:02.120 --> 01:05.640]  they're built on something called large language models,\n",
      "[01:06.180 --> 01:11.200]  or LLMs, that allow them to understand natural language commands.\n",
      "[01:11.360 --> 01:17.140]  Now, they rely on something called prompts from users to take action,\n",
      "[01:17.160 --> 01:20.480]  which means they need well-defined instructions.\n",
      "[01:21.260 --> 01:23.300]  And with those well-defined instructions,\n",
      "[01:23.300 --> 01:26.020]  AI assistants can make recommendations,\n",
      "[01:26.260 --> 01:28.760]  fetch information, and even generate content.\n",
      "[01:29.400 --> 01:32.680]  But they are always waiting for your input to get started.\n",
      "[01:33.100 --> 01:36.920]  And you have to continue to direct them, like a tennis match.\n",
      "[01:37.480 --> 01:41.840]  Prompt, response, prompt, response, prompt.\n",
      "[01:43.060 --> 01:44.800]  Response. Think I get it.\n",
      "[01:44.820 --> 01:50.080]  And there's a lot we can do to improve the quality of these prompt responses.\n",
      "[01:51.040 --> 01:56.040]  And for example, AI assistants may improve over time through prompt tuning,\n",
      "[01:56.200 --> 01:59.340]  which is adapting the underlying AI model for a specific task.\n",
      "[01:59.360 --> 02:02.840]  And we can also teach an assistant some new tricks.\n",
      "[02:03.220 --> 02:05.620]  That's called fine tuning.\n",
      "[02:05.720 --> 02:10.240]  We can fine tune these LLM models with specific examples.\n",
      "[02:11.000 --> 02:13.700]  In that way, it can get better at performing repetitive tasks,\n",
      "[02:13.840 --> 02:16.660]  like drafting documents based on patterns that it's learned.\n",
      "[02:17.380 --> 02:22.080]  Now, AI agents, on the other hand, they act independently.\n",
      "[02:22.320 --> 02:23.700]  You know, they take initiative.\n",
      "[02:24.040 --> 02:28.400]  They break down tasks and find the best way to achieve a goal. Right?\n",
      "[02:28.700 --> 02:32.780]  Right. They don't need the constant hand-holding of user prompts.\n",
      "[02:33.260 --> 02:37.260]  AI agents, they're still built on large language models,\n",
      "[02:37.260 --> 02:40.260]  but they can design their own workflows.\n",
      "[02:40.860 --> 02:43.920]  They do need a prompt, but just an initial prompt,\n",
      "[02:44.060 --> 02:46.180]  just one to get it started.\n",
      "[02:46.180 --> 02:48.280]  Telling the agent what we want it to do.\n",
      "[02:48.360 --> 02:53.940]  So, for example, one good prompt might be a goal of optimize our sales strategy.\n",
      "[02:54.220 --> 02:55.560]  The agent doesn't need further prompts.\n",
      "[02:55.580 --> 03:00.300]  It can use external data, tools and reasoning to make decisions autonomously.\n",
      "[03:01.140 --> 03:06.240]  So if AI assistants are your helpers taking care of routine tasks,\n",
      "[03:06.900 --> 03:11.680]  AI agents are your strategists, proactively driving outcomes.\n",
      "[03:12.040 --> 03:12.700]  Exactly, Amanda.\n",
      "[03:12.760 --> 03:14.220]  And to quote Elvis Presley,\n",
      "[03:14.220 --> 03:16.940]  a little less conversation, a little more action, please.\n",
      "[03:17.100 --> 03:18.180]  I thought you were going to do the voice.\n",
      "[03:18.340 --> 03:20.040]  You do not want to hear me doing the voice.\n",
      "[03:20.500 --> 03:23.040]  Now, AI agents, they can use external tools.\n",
      "[03:23.120 --> 03:24.940]  They can use external data sources,\n",
      "[03:25.220 --> 03:27.880]  whereas assistants, they depend on user input.\n",
      "[03:27.980 --> 03:31.120]  And agents can also have persistent memory,\n",
      "[03:31.400 --> 03:35.080]  meaning they remember past actions and improve future decisions\n",
      "[03:35.080 --> 03:36.540]  based on those experiences.\n",
      "[03:37.500 --> 03:39.840]  So let's compare use cases.\n",
      "[03:39.840 --> 03:44.020]  Assistance, they excel at tasks like customers.\n",
      "[03:44.860 --> 03:46.240]  I've got it. Customer service.\n",
      "[03:46.820 --> 03:47.640]  Yeah, customer service.\n",
      "[03:48.780 --> 03:54.260]  Virtual assistants, like chat bots and code generation.\n",
      "[03:55.720 --> 03:55.840]  Yeah, code generation.\n",
      "[03:55.840 --> 03:57.740]  Code generation, that is a good one.\n",
      "[03:57.860 --> 04:00.520]  So, for example, in customer service,\n",
      "[04:00.860 --> 04:04.580]  assistants use machine learning to quickly analyze large amounts of customer data\n",
      "[04:04.580 --> 04:05.600]  and then respond to queries,\n",
      "[04:05.600 --> 04:11.380]  often reducing the time that us humans need to spend on boring, repetitive tasks.\n",
      "[04:11.400 --> 04:12.140]  Thank goodness.\n",
      "[04:12.900 --> 04:16.840]  And AI agents, they thrive in more strategic roles,\n",
      "[04:17.080 --> 04:18.120]  like automated trading.\n",
      "[04:18.340 --> 04:19.840]  I got this, I got this one.\n",
      "[04:19.960 --> 04:24.420]  Like automated trading in finance and network monitoring.\n",
      "[04:24.880 --> 04:25.780]  Aha, yeah, right.\n",
      "[04:25.820 --> 04:29.240]  So let's take automated trading for an example.\n",
      "[04:29.600 --> 04:32.440]  AI agents analyze vast data sets filled with data\n",
      "[04:32.440 --> 04:34.820]  like historical trends and current news,\n",
      "[04:34.820 --> 04:38.000]  and then they use that information to extract insights.\n",
      "[04:38.280 --> 04:41.680]  And those insights predict how the market will behave and execute trades\n",
      "[04:41.680 --> 04:44.940]  in real time based upon those predictive algorithms.\n",
      "[04:45.800 --> 04:46.300]  Super cool.\n",
      "[04:47.000 --> 04:50.180]  So assistants, they handle your routine tasks,\n",
      "[04:50.400 --> 04:54.300]  while agents, they take on more complex, high-level challenges.\n",
      "[04:54.780 --> 04:59.980]  And AI agents, they can scale across multiple tasks all at the same time\n",
      "[04:59.980 --> 05:01.600]  without any human intervention,\n",
      "[05:01.600 --> 05:05.480]  making them ideal for dynamic and ambiguous problems.\n",
      "[05:06.200 --> 05:07.300]  Speaking of problems, though,\n",
      "[05:07.900 --> 05:11.940]  AI assistants and AI agents can have limitations,\n",
      "[05:12.360 --> 05:16.800]  like brittleness, when slight changes to prompts lead to errors.\n",
      "[05:17.020 --> 05:19.180]  Yes, beware the rabbit hole.\n",
      "[05:19.540 --> 05:22.880]  AI agents in particular may get stuck in feedback loops,\n",
      "[05:23.460 --> 05:26.200]  or they might require significant computational resources,\n",
      "[05:26.260 --> 05:28.100]  making them expensive to run.\n",
      "[05:28.100 --> 05:33.460]  Without supervision, they can go down all sorts of weird and wonderful paths.\n",
      "[05:34.400 --> 05:36.460]  Always good to check those AI outputs.\n",
      "[05:36.920 --> 05:38.960]  But there's improvement happening every day,\n",
      "[05:39.460 --> 05:42.300]  and as AI agents improve, we'll likely see them\n",
      "[05:42.300 --> 05:46.020]  tackling even more complex problems without needing human assistance.\n",
      "[05:46.340 --> 05:49.120]  And we're already seeing improvements in model reasoning,\n",
      "[05:49.280 --> 05:53.900]  meaning that AI agents will become more reliable and effective over time.\n",
      "[05:53.960 --> 05:57.360]  Take, for example, OpenAI's O1 model,\n",
      "[05:57.360 --> 06:00.020]  which performs reasoning at inference time.\n",
      "[06:00.820 --> 06:05.840]  So, to recap, AI assistants, they help with your day-to-day tasks,\n",
      "[06:06.280 --> 06:10.760]  while AI agents take on a more autonomous approach to problem solving.\n",
      "[06:11.160 --> 06:13.680]  And it's not necessarily an either-or here,\n",
      "[06:13.720 --> 06:15.620]  because as these technologies evolve,\n",
      "[06:16.160 --> 06:19.880]  expect to see more synergy between assistants and agents,\n",
      "[06:20.040 --> 06:24.260]  combining their strengths to tackle both simple and complex tasks.\n",
      "[06:24.260 --> 06:30.060]  It's the movie star's assistant and agent not only working together,\n",
      "[06:30.300 --> 06:34.460]  but also knowing the best way to do so, even if you don't.\n",
      "[06:34.740 --> 06:35.400]  Nice!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:02:16.584501Z",
     "start_time": "2024-11-22T21:02:16.581110Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.keys())",
   "id": "e836d241f6e28f69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'segments', 'language'])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:02:29.911516Z",
     "start_time": "2024-11-22T21:02:29.908588Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('text'))",
   "id": "39b3ddba8e5f01e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imagine you're a movie star with both an assistant and an agent. You know, I was chatting to both of mine just yesterday. Sure you were. Now, assistants, they help with scheduling, and agents are more proactive, seeking opportunities for you. Artificial intelligence works much the same way. You have AI assistants and AI agents. And in this video, we are going to explore how both of these types of AI are shaping the future of work. Right. And at a fundamental level, I think we can say the main difference is that AI assistants, they're reactive, right? They're waiting for commands like a prompt from the user, while AI agents are proactive. They're acting autonomously to achieve a goal. So, when you say we get into it. Let's do it. So let's start with AI assistants. These helpful little apps understand natural language, and they're great for doing things like organizing information or responding to customer queries. Examples that we know are Siri, Alexa, or chat GPT. Right, and most AI assistants, they're built on something called large language models, or LLMs, that allow them to understand natural language commands. Now, they rely on something called prompts from users to take action, which means they need well-defined instructions. And with those well-defined instructions, AI assistants can make recommendations, fetch information, and even generate content. But they are always waiting for your input to get started. And you have to continue to direct them, like a tennis match. Prompt, response, prompt, response, prompt. Response. Think I get it. And there's a lot we can do to improve the quality of these prompt responses. And for example, AI assistants may improve over time through prompt tuning, which is adapting the underlying AI model for a specific task. And we can also teach an assistant some new tricks. That's called fine tuning. We can fine tune these LLM models with specific examples. In that way, it can get better at performing repetitive tasks, like drafting documents based on patterns that it's learned. Now, AI agents, on the other hand, they act independently. You know, they take initiative. They break down tasks and find the best way to achieve a goal. Right? Right. They don't need the constant hand-holding of user prompts. AI agents, they're still built on large language models, but they can design their own workflows. They do need a prompt, but just an initial prompt, just one to get it started. Telling the agent what we want it to do. So, for example, one good prompt might be a goal of optimize our sales strategy. The agent doesn't need further prompts. It can use external data, tools and reasoning to make decisions autonomously. So if AI assistants are your helpers taking care of routine tasks, AI agents are your strategists, proactively driving outcomes. Exactly, Amanda. And to quote Elvis Presley, a little less conversation, a little more action, please. I thought you were going to do the voice. You do not want to hear me doing the voice. Now, AI agents, they can use external tools. They can use external data sources, whereas assistants, they depend on user input. And agents can also have persistent memory, meaning they remember past actions and improve future decisions based on those experiences. So let's compare use cases. Assistance, they excel at tasks like customers. I've got it. Customer service. Yeah, customer service. Virtual assistants, like chat bots and code generation. Yeah, code generation. Code generation, that is a good one. So, for example, in customer service, assistants use machine learning to quickly analyze large amounts of customer data and then respond to queries, often reducing the time that us humans need to spend on boring, repetitive tasks. Thank goodness. And AI agents, they thrive in more strategic roles, like automated trading. I got this, I got this one. Like automated trading in finance and network monitoring. Aha, yeah, right. So let's take automated trading for an example. AI agents analyze vast data sets filled with data like historical trends and current news, and then they use that information to extract insights. And those insights predict how the market will behave and execute trades in real time based upon those predictive algorithms. Super cool. So assistants, they handle your routine tasks, while agents, they take on more complex, high-level challenges. And AI agents, they can scale across multiple tasks all at the same time without any human intervention, making them ideal for dynamic and ambiguous problems. Speaking of problems, though, AI assistants and AI agents can have limitations, like brittleness, when slight changes to prompts lead to errors. Yes, beware the rabbit hole. AI agents in particular may get stuck in feedback loops, or they might require significant computational resources, making them expensive to run. Without supervision, they can go down all sorts of weird and wonderful paths. Always good to check those AI outputs. But there's improvement happening every day, and as AI agents improve, we'll likely see them tackling even more complex problems without needing human assistance. And we're already seeing improvements in model reasoning, meaning that AI agents will become more reliable and effective over time. Take, for example, OpenAI's O1 model, which performs reasoning at inference time. So, to recap, AI assistants, they help with your day-to-day tasks, while AI agents take on a more autonomous approach to problem solving. And it's not necessarily an either-or here, because as these technologies evolve, expect to see more synergy between assistants and agents, combining their strengths to tackle both simple and complex tasks. It's the movie star's assistant and agent not only working together, but also knowing the best way to do so, even if you don't. Nice!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:07:32.486867Z",
     "start_time": "2024-11-22T21:07:32.484152Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('segments')[1].keys())",
   "id": "a7bbc2c30933a00e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'seek', 'start', 'end', 'text', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob', 'words'])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:08:15.570722Z",
     "start_time": "2024-11-22T21:08:15.567692Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('segments')[1].get('text'))",
   "id": "fc289e9732bc4053",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You know, I was chatting to both of mine just yesterday.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:08:49.827597Z",
     "start_time": "2024-11-22T21:08:49.824135Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('segments')[1].get('start'), \"->\", result.get('segments')[1].get('end'))",
   "id": "f6f20331701e8d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.84 -> 5.9\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:09:14.953031Z",
     "start_time": "2024-11-22T21:09:14.950233Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('segments')[1].get('words')[1].keys())",
   "id": "2f7f03365ad2a4e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['word', 'start', 'end', 'probability'])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:10:35.994076Z",
     "start_time": "2024-11-22T21:10:35.988863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in result.get('segments')[1].get('words'):\n",
    "    print(f'START: {word[\"start\"]} :: END: {word[\"end\"]} - WORD: {word[\"word\"]}, PROBABILITY: {word[\"probability\"]}')"
   ],
   "id": "f3949bc079c33fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: 3.84 :: END: 3.9 - WORD:  You, PROBABILITY: 0.6495263576507568\n",
      "START: 3.9 :: END: 4.06 - WORD:  know,, PROBABILITY: 0.9661267399787903\n",
      "START: 4.18 :: END: 4.3 - WORD:  I, PROBABILITY: 0.9629467129707336\n",
      "START: 4.3 :: END: 4.4 - WORD:  was, PROBABILITY: 0.9335654973983765\n",
      "START: 4.4 :: END: 4.44 - WORD:  chatting, PROBABILITY: 0.5326090455055237\n",
      "START: 4.44 :: END: 4.62 - WORD:  to, PROBABILITY: 0.9845169186592102\n",
      "START: 4.62 :: END: 4.84 - WORD:  both, PROBABILITY: 0.9938716888427734\n",
      "START: 4.84 :: END: 5.0 - WORD:  of, PROBABILITY: 0.9958957433700562\n",
      "START: 5.0 :: END: 5.16 - WORD:  mine, PROBABILITY: 0.9756839871406555\n",
      "START: 5.16 :: END: 5.4 - WORD:  just, PROBABILITY: 0.9744288921356201\n",
      "START: 5.4 :: END: 5.9 - WORD:  yesterday., PROBABILITY: 0.9969227910041809\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
