{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:36.192526Z",
     "start_time": "2024-11-22T20:52:36.189376Z"
    }
   },
   "source": "from whisper_transcriber import WhisperTranscriber, TranscriptionConfig",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:36.201052Z",
     "start_time": "2024-11-22T20:52:36.198591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Yapılandırma oluştur\n",
    "config = TranscriptionConfig(\n",
    "    model_name='base',      # Kullanılacak Whisper modeli\n",
    "    verbose=False           # Ayrıntılı çıktıyı devre dışı bırak\n",
    ")"
   ],
   "id": "6d0f9f9193291fdd",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:37.289632Z",
     "start_time": "2024-11-22T20:52:36.208681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transkriber'ı başlat\n",
    "transcriber = WhisperTranscriber(config)"
   ],
   "id": "18c6e8c9e6a08901",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enesa/PycharmProjects/WhisperDemo/.venv/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:55.885695Z",
     "start_time": "2024-11-22T20:52:37.294998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ses dosyasını dökümleştir\n",
    "result = transcriber.transcribe('demo.mp3')"
   ],
   "id": "18f53ad688d83f6a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enesa/PycharmProjects/WhisperDemo/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40857/40857 [00:17<00:00, 2296.68frames/s]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:55.893511Z",
     "start_time": "2024-11-22T20:52:55.891389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transkripsiyon sonuçlarını al\n",
    "print(result.keys())  # Tam transkripsiyon metni"
   ],
   "id": "76aacfa377a817cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'segments', 'language'])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:55.903149Z",
     "start_time": "2024-11-22T20:52:55.901190Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('text'))",
   "id": "9fabfe9d87b62e8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imagine your movie star with both an assistant and an agent. You know us chatting to both of mine just yesterday. Sure you are. Now, assistants, they help with scheduling and agents are more proactive seeking opportunities for you. Artificial intelligence works much the same way. You have AI assistants and AI agents. In this video, we are going to explore how both of these types of AI are shaping the future of work. Right. And at a fundamental level, I think we can say the main difference is that AI assistants are reactive. They are waiting for commands like a prompt from the user while AI agents are proactive. They are acting autonomously to achieve a goal. So, when we say we get into it. Let's do it. So let's start with AI assistants. These helpful little apps understand natural language and they are great for doing things like organizing information or responding to customer queries. But examples that we know are Siri, Alexa or ChatGPT. Right. And most AI assistants, they are built on something called large language models or LLMs that allow them to understand natural language commands. Now, they rely on something called prompts from users to take action, which means they need well-defined instructions. And with those well-defined instructions, AI assistants can make recommendations, fetch information, and even generate content. But they are always waiting for your input to get started. And you have to continue to direct them like a tennis match. Prompt. Response. Prompt. Response. Prompt. Response. Think I get it. And there's a lot we can do to improve the quality of these prompt-tem responses. And for example, AI assistants may improve over time through prompt tuning, which is adapting the underlying AI model for a specific task. And we can also teach an assistant some new tricks. That's called fine tuning. We can fine tune these LLMs models with specific examples. In that way, it can get better at performing repetitive tasks like drafting documents based on patterns that it's learned. Now, AI agents, on the other hand, they act independently. You know, they take initiative. They break down tasks and find the best way to achieve a goal, right? Right, they don't need the constant hand holding of user promise. AI agents, they're still built on large language models, but they can design their own workflows. They do need a prompt, but just an initial prompt. Just one to get it started. Telling the agent what we want it to do. So for example, one good prompt might be a goal of optimize our sales strategy. The agent doesn't need further promise. It can use external data, tools, and reasoning to make decisions autonomously. So if AI assistants are your helpers, taking care of routine tasks, AI agents are your strategists, proactively driving outcomes. Exactly Amanda, and to quote Elvis Presley, a little less conversation, a little more action please. I thought you were going to do the voice. You do not want to hear me doing the voice. Now, AI agents, they can use external tools, they can use external data sources, whereas assistants, they depend on user input. And agents can also have persistent memory, meaning they remember past actions and improve future decisions based on those experiences. So let's compare use cases. Assistance, they excel at tasks like customers. I got a customer service. Yeah, customer service. Okay. Virtual assistants, chatbots, and code generation. Yeah, code gen. That is a good one. So for example, in customer service, assistants use machine learning to quickly analyze large amounts of customer data and then respond to queries, often reducing the time that us humans need to spend on boring repetitive tasks. Thank goodness. And AI agents, they thrive in more strategic roles, like automated trading. If I got this, I got this one. Like automated trading in finance and network monitoring. Aha, yeah. Right. So let's take automated trading, for example. AI agents analyze vast data sets filled with data like historical trends and current news. And then they use that information to extract insights. And those insights predict how the market will behave and execute trades in real time based upon those predictive algorithms. Super cool. So assistance, they handle your routine tasks. While agents, they take on more complex high-level challenges. And AI agents, they can scale across multiple tasks all of the same time without any human intervention, making them ideal for dynamic and ambiguous problems. Speaking of problems, though. AI assistants and AI agents can have limitations like brittleness. When slight changes to prompts lead to errors. Yes, beware the rabbit hole. AI agents in particular make it stuck in feedback loops. Or they might require significant computational resources, making them expensive to run. Without supervision, they can go down all sorts of weird and wonderful paths. Always good to check those AI outputs. But there's improvement happening every day. And as AI agents improve, we'll likely see them tackling even more complex problems without needing human assistance. And we're already seeing improvements in model reasoning. Meaning that AI agents will become more reliable and effective over time. Take, for example, open AI's O1 model, which performs reasoning at inference time. So to recap, AI assistants, they help with your day-to-day tasks. While AI agents take on a more autonomous approach to problem solving. And it's not necessarily an either or here. Because as these technologies evolve, expect to see more synergy between assistance and agents combining their strengths to tackle both simple and complex tasks. It's the movie stars, assistant and agent. Not only working together, but also knowing the best way to do so. Even if you don't. Nice. Oh god.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:07:53.285250Z",
     "start_time": "2024-11-22T21:07:53.278123Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('segments')[1].keys())",
   "id": "92b0fe9a551bdda1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'seek', 'start', 'end', 'text', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob'])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:55.912624Z",
     "start_time": "2024-11-22T20:52:55.910143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for segment in result.get('segments'):\n",
    "    print(segment)"
   ],
   "id": "a407276afc70f386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'seek': 0, 'start': 0.0, 'end': 3.68, 'text': ' Imagine your movie star with both an assistant and an agent.', 'tokens': [50364, 11739, 428, 3169, 3543, 365, 1293, 364, 10994, 293, 364, 9461, 13, 50548], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 1, 'seek': 0, 'start': 3.68, 'end': 6.32, 'text': ' You know us chatting to both of mine just yesterday.', 'tokens': [50548, 509, 458, 505, 24654, 281, 1293, 295, 3892, 445, 5186, 13, 50680], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 2, 'seek': 0, 'start': 6.32, 'end': 7.5200000000000005, 'text': ' Sure you are.', 'tokens': [50680, 4894, 291, 366, 13, 50740], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 3, 'seek': 0, 'start': 7.5200000000000005, 'end': 15.4, 'text': ' Now, assistants, they help with scheduling and agents are more proactive seeking opportunities for you.', 'tokens': [50740, 823, 11, 34949, 11, 436, 854, 365, 29055, 293, 12554, 366, 544, 28028, 11670, 4786, 337, 291, 13, 51134], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 4, 'seek': 0, 'start': 15.4, 'end': 17.92, 'text': ' Artificial intelligence works much the same way.', 'tokens': [51134, 5735, 10371, 7599, 1985, 709, 264, 912, 636, 13, 51260], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 5, 'seek': 0, 'start': 17.92, 'end': 20.96, 'text': ' You have AI assistants and AI agents.', 'tokens': [51260, 509, 362, 7318, 34949, 293, 7318, 12554, 13, 51412], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 6, 'seek': 0, 'start': 20.96, 'end': 27.0, 'text': ' In this video, we are going to explore how both of these types of AI are shaping the future of work.', 'tokens': [51412, 682, 341, 960, 11, 321, 366, 516, 281, 6839, 577, 1293, 295, 613, 3467, 295, 7318, 366, 25945, 264, 2027, 295, 589, 13, 51714], 'temperature': 0.0, 'avg_logprob': -0.2608264129940826, 'compression_ratio': 1.63671875, 'no_speech_prob': 0.10574696958065033}\n",
      "{'id': 7, 'seek': 2700, 'start': 27.0, 'end': 32.92, 'text': ' Right. And at a fundamental level, I think we can say the main difference is that AI assistants are reactive.', 'tokens': [50364, 1779, 13, 400, 412, 257, 8088, 1496, 11, 286, 519, 321, 393, 584, 264, 2135, 2649, 307, 300, 7318, 34949, 366, 28897, 13, 50660], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 8, 'seek': 2700, 'start': 32.92, 'end': 38.76, 'text': ' They are waiting for commands like a prompt from the user while AI agents are proactive.', 'tokens': [50660, 814, 366, 3806, 337, 16901, 411, 257, 12391, 490, 264, 4195, 1339, 7318, 12554, 366, 28028, 13, 50952], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 9, 'seek': 2700, 'start': 38.76, 'end': 40.6, 'text': ' They are acting autonomously to achieve a goal.', 'tokens': [50952, 814, 366, 6577, 18203, 5098, 281, 4584, 257, 3387, 13, 51044], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 10, 'seek': 2700, 'start': 40.6, 'end': 42.519999999999996, 'text': ' So, when we say we get into it.', 'tokens': [51044, 407, 11, 562, 321, 584, 321, 483, 666, 309, 13, 51140], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 11, 'seek': 2700, 'start': 42.519999999999996, 'end': 43.72, 'text': \" Let's do it.\", 'tokens': [51140, 961, 311, 360, 309, 13, 51200], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 12, 'seek': 2700, 'start': 43.72, 'end': 46.92, 'text': \" So let's start with AI assistants.\", 'tokens': [51200, 407, 718, 311, 722, 365, 7318, 34949, 13, 51360], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 13, 'seek': 2700, 'start': 46.92, 'end': 51.56, 'text': ' These helpful little apps understand natural language and they are great', 'tokens': [51360, 1981, 4961, 707, 7733, 1223, 3303, 2856, 293, 436, 366, 869, 51592], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 14, 'seek': 2700, 'start': 51.56, 'end': 56.36, 'text': ' for doing things like organizing information or responding to customer queries.', 'tokens': [51592, 337, 884, 721, 411, 17608, 1589, 420, 16670, 281, 5474, 24109, 13, 51832], 'temperature': 0.0, 'avg_logprob': -0.30366415726511103, 'compression_ratio': 1.6748251748251748, 'no_speech_prob': 0.021920958533883095}\n",
      "{'id': 15, 'seek': 5636, 'start': 56.36, 'end': 60.36, 'text': ' But examples that we know are Siri, Alexa or ChatGPT.', 'tokens': [50364, 583, 5110, 300, 321, 458, 366, 33682, 11, 22595, 420, 27503, 38, 47, 51, 13, 50564], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 16, 'seek': 5636, 'start': 60.36, 'end': 67.32, 'text': ' Right. And most AI assistants, they are built on something called large language models or LLMs', 'tokens': [50564, 1779, 13, 400, 881, 7318, 34949, 11, 436, 366, 3094, 322, 746, 1219, 2416, 2856, 5245, 420, 441, 43, 26386, 50912], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 17, 'seek': 5636, 'start': 67.32, 'end': 71.32, 'text': ' that allow them to understand natural language commands.', 'tokens': [50912, 300, 2089, 552, 281, 1223, 3303, 2856, 16901, 13, 51112], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 18, 'seek': 5636, 'start': 71.32, 'end': 77.16, 'text': ' Now, they rely on something called prompts from users to take action,', 'tokens': [51112, 823, 11, 436, 10687, 322, 746, 1219, 41095, 490, 5022, 281, 747, 3069, 11, 51404], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 19, 'seek': 5636, 'start': 77.16, 'end': 80.92, 'text': ' which means they need well-defined instructions.', 'tokens': [51404, 597, 1355, 436, 643, 731, 12, 37716, 9415, 13, 51592], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 20, 'seek': 5636, 'start': 80.92, 'end': 83.4, 'text': ' And with those well-defined instructions,', 'tokens': [51592, 400, 365, 729, 731, 12, 37716, 9415, 11, 51716], 'temperature': 0.0, 'avg_logprob': -0.23080669360214404, 'compression_ratio': 1.6311111111111112, 'no_speech_prob': 0.019071733579039574}\n",
      "{'id': 21, 'seek': 8340, 'start': 83.4, 'end': 88.52000000000001, 'text': ' AI assistants can make recommendations, fetch information, and even generate content.', 'tokens': [50364, 7318, 34949, 393, 652, 10434, 11, 23673, 1589, 11, 293, 754, 8460, 2701, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 22, 'seek': 8340, 'start': 89.24000000000001, 'end': 93.0, 'text': ' But they are always waiting for your input to get started.', 'tokens': [50656, 583, 436, 366, 1009, 3806, 337, 428, 4846, 281, 483, 1409, 13, 50844], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 23, 'seek': 8340, 'start': 93.0, 'end': 96.36000000000001, 'text': ' And you have to continue to direct them like a tennis match.', 'tokens': [50844, 400, 291, 362, 281, 2354, 281, 2047, 552, 411, 257, 18118, 2995, 13, 51012], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 24, 'seek': 8340, 'start': 97.0, 'end': 97.80000000000001, 'text': ' Prompt.', 'tokens': [51044, 15833, 662, 13, 51084], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 25, 'seek': 8340, 'start': 97.80000000000001, 'end': 98.52000000000001, 'text': ' Response.', 'tokens': [51084, 43937, 13, 51120], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 26, 'seek': 8340, 'start': 99.0, 'end': 99.80000000000001, 'text': ' Prompt.', 'tokens': [51144, 15833, 662, 13, 51184], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 27, 'seek': 8340, 'start': 99.80000000000001, 'end': 100.60000000000001, 'text': ' Response.', 'tokens': [51184, 43937, 13, 51224], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 28, 'seek': 8340, 'start': 101.16000000000001, 'end': 101.64000000000001, 'text': ' Prompt.', 'tokens': [51252, 15833, 662, 13, 51276], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 29, 'seek': 8340, 'start': 102.84, 'end': 103.80000000000001, 'text': ' Response.', 'tokens': [51336, 43937, 13, 51384], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 30, 'seek': 8340, 'start': 103.80000000000001, 'end': 104.68, 'text': ' Think I get it.', 'tokens': [51384, 6557, 286, 483, 309, 13, 51428], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 31, 'seek': 8340, 'start': 104.68, 'end': 110.2, 'text': \" And there's a lot we can do to improve the quality of these prompt-tem responses.\", 'tokens': [51428, 400, 456, 311, 257, 688, 321, 393, 360, 281, 3470, 264, 3125, 295, 613, 12391, 12, 18275, 13019, 13, 51704], 'temperature': 0.0, 'avg_logprob': -0.21076376297894647, 'compression_ratio': 1.6604651162790698, 'no_speech_prob': 0.006554673425853252}\n",
      "{'id': 32, 'seek': 11020, 'start': 111.0, 'end': 115.56, 'text': ' And for example, AI assistants may improve over time through prompt', 'tokens': [50404, 400, 337, 1365, 11, 7318, 34949, 815, 3470, 670, 565, 807, 12391, 50632], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 33, 'seek': 11020, 'start': 115.56, 'end': 119.24000000000001, 'text': ' tuning, which is adapting the underlying AI model for a specific task.', 'tokens': [50632, 15164, 11, 597, 307, 34942, 264, 14217, 7318, 2316, 337, 257, 2685, 5633, 13, 50816], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 34, 'seek': 11020, 'start': 119.24000000000001, 'end': 123.08, 'text': ' And we can also teach an assistant some new tricks.', 'tokens': [50816, 400, 321, 393, 611, 2924, 364, 10994, 512, 777, 11733, 13, 51008], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 35, 'seek': 11020, 'start': 123.08, 'end': 125.56, 'text': \" That's called fine tuning.\", 'tokens': [51008, 663, 311, 1219, 2489, 15164, 13, 51132], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 36, 'seek': 11020, 'start': 125.56, 'end': 130.12, 'text': ' We can fine tune these LLMs models with specific examples.', 'tokens': [51132, 492, 393, 2489, 10864, 613, 441, 43, 26386, 5245, 365, 2685, 5110, 13, 51360], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 37, 'seek': 11020, 'start': 130.76, 'end': 134.92000000000002, 'text': ' In that way, it can get better at performing repetitive tasks like drafting documents', 'tokens': [51392, 682, 300, 636, 11, 309, 393, 483, 1101, 412, 10205, 29404, 9608, 411, 46378, 8512, 51600], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 38, 'seek': 11020, 'start': 134.92000000000002, 'end': 136.52, 'text': \" based on patterns that it's learned.\", 'tokens': [51600, 2361, 322, 8294, 300, 309, 311, 3264, 13, 51680], 'temperature': 0.0, 'avg_logprob': -0.13747159054404812, 'compression_ratio': 1.596, 'no_speech_prob': 0.08144495636224747}\n",
      "{'id': 39, 'seek': 13652, 'start': 137.32000000000002, 'end': 142.12, 'text': ' Now, AI agents, on the other hand, they act independently.', 'tokens': [50404, 823, 11, 7318, 12554, 11, 322, 264, 661, 1011, 11, 436, 605, 21761, 13, 50644], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 40, 'seek': 13652, 'start': 142.12, 'end': 143.88000000000002, 'text': ' You know, they take initiative.', 'tokens': [50644, 509, 458, 11, 436, 747, 11552, 13, 50732], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 41, 'seek': 13652, 'start': 143.88000000000002, 'end': 148.52, 'text': ' They break down tasks and find the best way to achieve a goal, right?', 'tokens': [50732, 814, 1821, 760, 9608, 293, 915, 264, 1151, 636, 281, 4584, 257, 3387, 11, 558, 30, 50964], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 42, 'seek': 13652, 'start': 148.52, 'end': 153.0, 'text': \" Right, they don't need the constant hand holding of user promise.\", 'tokens': [50964, 1779, 11, 436, 500, 380, 643, 264, 5754, 1011, 5061, 295, 4195, 6228, 13, 51188], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 43, 'seek': 13652, 'start': 153.0, 'end': 157.48000000000002, 'text': \" AI agents, they're still built on large language models,\", 'tokens': [51188, 7318, 12554, 11, 436, 434, 920, 3094, 322, 2416, 2856, 5245, 11, 51412], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 44, 'seek': 13652, 'start': 157.48000000000002, 'end': 160.20000000000002, 'text': ' but they can design their own workflows.', 'tokens': [51412, 457, 436, 393, 1715, 641, 1065, 43461, 13, 51548], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 45, 'seek': 13652, 'start': 160.76000000000002, 'end': 163.88, 'text': ' They do need a prompt, but just an initial prompt.', 'tokens': [51576, 814, 360, 643, 257, 12391, 11, 457, 445, 364, 5883, 12391, 13, 51732], 'temperature': 0.0, 'avg_logprob': -0.10778535014451152, 'compression_ratio': 1.609442060085837, 'no_speech_prob': 0.04918202385306358}\n",
      "{'id': 46, 'seek': 16388, 'start': 163.96, 'end': 166.28, 'text': ' Just one to get it started.', 'tokens': [50368, 1449, 472, 281, 483, 309, 1409, 13, 50484], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 47, 'seek': 16388, 'start': 166.28, 'end': 168.12, 'text': ' Telling the agent what we want it to do.', 'tokens': [50484, 5115, 278, 264, 9461, 437, 321, 528, 309, 281, 360, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 48, 'seek': 16388, 'start': 168.12, 'end': 174.04, 'text': ' So for example, one good prompt might be a goal of optimize our sales strategy.', 'tokens': [50576, 407, 337, 1365, 11, 472, 665, 12391, 1062, 312, 257, 3387, 295, 19719, 527, 5763, 5206, 13, 50872], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 49, 'seek': 16388, 'start': 174.04, 'end': 175.48, 'text': \" The agent doesn't need further promise.\", 'tokens': [50872, 440, 9461, 1177, 380, 643, 3052, 6228, 13, 50944], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 50, 'seek': 16388, 'start': 175.48, 'end': 180.04, 'text': ' It can use external data, tools, and reasoning to make decisions autonomously.', 'tokens': [50944, 467, 393, 764, 8320, 1412, 11, 3873, 11, 293, 21577, 281, 652, 5327, 18203, 5098, 13, 51172], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 51, 'seek': 16388, 'start': 180.84, 'end': 184.44, 'text': ' So if AI assistants are your helpers,', 'tokens': [51212, 407, 498, 7318, 34949, 366, 428, 854, 433, 11, 51392], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 52, 'seek': 16388, 'start': 184.44, 'end': 189.4, 'text': ' taking care of routine tasks, AI agents are your strategists,', 'tokens': [51392, 1940, 1127, 295, 9927, 9608, 11, 7318, 12554, 366, 428, 5464, 1751, 11, 51640], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 53, 'seek': 16388, 'start': 189.4, 'end': 191.8, 'text': ' proactively driving outcomes.', 'tokens': [51640, 447, 45679, 4840, 10070, 13, 51760], 'temperature': 0.0, 'avg_logprob': -0.14882648908174956, 'compression_ratio': 1.5816733067729083, 'no_speech_prob': 0.0021685839165002108}\n",
      "{'id': 54, 'seek': 19180, 'start': 191.88000000000002, 'end': 194.36, 'text': ' Exactly Amanda, and to quote Elvis Presley,', 'tokens': [50368, 7587, 20431, 11, 293, 281, 6513, 39944, 2718, 3420, 11, 50492], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 55, 'seek': 19180, 'start': 194.36, 'end': 196.92000000000002, 'text': ' a little less conversation, a little more action please.', 'tokens': [50492, 257, 707, 1570, 3761, 11, 257, 707, 544, 3069, 1767, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 56, 'seek': 19180, 'start': 196.92000000000002, 'end': 198.20000000000002, 'text': ' I thought you were going to do the voice.', 'tokens': [50620, 286, 1194, 291, 645, 516, 281, 360, 264, 3177, 13, 50684], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 57, 'seek': 19180, 'start': 198.20000000000002, 'end': 200.04000000000002, 'text': ' You do not want to hear me doing the voice.', 'tokens': [50684, 509, 360, 406, 528, 281, 1568, 385, 884, 264, 3177, 13, 50776], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 58, 'seek': 19180, 'start': 200.04000000000002, 'end': 203.0, 'text': ' Now, AI agents, they can use external tools,', 'tokens': [50776, 823, 11, 7318, 12554, 11, 436, 393, 764, 8320, 3873, 11, 50924], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 59, 'seek': 19180, 'start': 203.0, 'end': 204.92000000000002, 'text': ' they can use external data sources,', 'tokens': [50924, 436, 393, 764, 8320, 1412, 7139, 11, 51020], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 60, 'seek': 19180, 'start': 204.92000000000002, 'end': 207.8, 'text': ' whereas assistants, they depend on user input.', 'tokens': [51020, 9735, 34949, 11, 436, 5672, 322, 4195, 4846, 13, 51164], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 61, 'seek': 19180, 'start': 207.8, 'end': 211.24, 'text': ' And agents can also have persistent memory,', 'tokens': [51164, 400, 12554, 393, 611, 362, 24315, 4675, 11, 51336], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 62, 'seek': 19180, 'start': 211.24, 'end': 215.24, 'text': ' meaning they remember past actions and improve future decisions', 'tokens': [51336, 3620, 436, 1604, 1791, 5909, 293, 3470, 2027, 5327, 51536], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 63, 'seek': 19180, 'start': 215.24, 'end': 216.52, 'text': ' based on those experiences.', 'tokens': [51536, 2361, 322, 729, 5235, 13, 51600], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 64, 'seek': 19180, 'start': 217.24, 'end': 219.72000000000003, 'text': \" So let's compare use cases.\", 'tokens': [51636, 407, 718, 311, 6794, 764, 3331, 13, 51760], 'temperature': 0.0, 'avg_logprob': -0.11700458213931224, 'compression_ratio': 1.707142857142857, 'no_speech_prob': 0.007828894071280956}\n",
      "{'id': 65, 'seek': 21972, 'start': 220.28, 'end': 223.88, 'text': ' Assistance, they excel at tasks like customers.', 'tokens': [50392, 46805, 11, 436, 24015, 412, 9608, 411, 4581, 13, 50572], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 66, 'seek': 21972, 'start': 224.76, 'end': 226.2, 'text': ' I got a customer service.', 'tokens': [50616, 286, 658, 257, 5474, 2643, 13, 50688], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 67, 'seek': 21972, 'start': 226.2, 'end': 227.64, 'text': ' Yeah, customer service.', 'tokens': [50688, 865, 11, 5474, 2643, 13, 50760], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 68, 'seek': 21972, 'start': 227.64, 'end': 228.36, 'text': ' Okay.', 'tokens': [50760, 1033, 13, 50796], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 69, 'seek': 21972, 'start': 228.36, 'end': 231.32, 'text': ' Virtual assistants, chatbots,', 'tokens': [50796, 23887, 34949, 11, 5081, 65, 1971, 11, 50944], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 70, 'seek': 21972, 'start': 232.68, 'end': 234.28, 'text': ' and code generation.', 'tokens': [51012, 293, 3089, 5125, 13, 51092], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 71, 'seek': 21972, 'start': 234.28, 'end': 236.44, 'text': ' Yeah, code gen.', 'tokens': [51092, 865, 11, 3089, 1049, 13, 51200], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 72, 'seek': 21972, 'start': 236.44, 'end': 237.64, 'text': ' That is a good one.', 'tokens': [51200, 663, 307, 257, 665, 472, 13, 51260], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 73, 'seek': 21972, 'start': 237.64, 'end': 240.44, 'text': ' So for example, in customer service,', 'tokens': [51260, 407, 337, 1365, 11, 294, 5474, 2643, 11, 51400], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 74, 'seek': 21972, 'start': 240.44, 'end': 244.44, 'text': ' assistants use machine learning to quickly analyze large amounts of customer data', 'tokens': [51400, 34949, 764, 3479, 2539, 281, 2661, 12477, 2416, 11663, 295, 5474, 1412, 51600], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 75, 'seek': 21972, 'start': 244.44, 'end': 245.88, 'text': ' and then respond to queries,', 'tokens': [51600, 293, 550, 4196, 281, 24109, 11, 51672], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 76, 'seek': 21972, 'start': 245.88, 'end': 249.48, 'text': ' often reducing the time that us humans need to spend', 'tokens': [51672, 2049, 12245, 264, 565, 300, 505, 6255, 643, 281, 3496, 51852], 'temperature': 0.0, 'avg_logprob': -0.322086586142486, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.03238850459456444}\n",
      "{'id': 77, 'seek': 24948, 'start': 249.56, 'end': 251.48, 'text': ' on boring repetitive tasks.', 'tokens': [50368, 322, 9989, 29404, 9608, 13, 50464], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 78, 'seek': 24948, 'start': 251.48, 'end': 252.51999999999998, 'text': ' Thank goodness.', 'tokens': [50464, 1044, 8387, 13, 50516], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 79, 'seek': 24948, 'start': 252.51999999999998, 'end': 256.84, 'text': ' And AI agents, they thrive in more strategic roles,', 'tokens': [50516, 400, 7318, 12554, 11, 436, 21233, 294, 544, 10924, 9604, 11, 50732], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 80, 'seek': 24948, 'start': 256.84, 'end': 258.03999999999996, 'text': ' like automated trading.', 'tokens': [50732, 411, 18473, 9529, 13, 50792], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 81, 'seek': 24948, 'start': 258.03999999999996, 'end': 259.8, 'text': ' If I got this, I got this one.', 'tokens': [50792, 759, 286, 658, 341, 11, 286, 658, 341, 472, 13, 50880], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 82, 'seek': 24948, 'start': 259.8, 'end': 264.68, 'text': ' Like automated trading in finance and network monitoring.', 'tokens': [50880, 1743, 18473, 9529, 294, 10719, 293, 3209, 11028, 13, 51124], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 83, 'seek': 24948, 'start': 264.68, 'end': 265.4, 'text': ' Aha, yeah.', 'tokens': [51124, 27448, 11, 1338, 13, 51160], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 84, 'seek': 24948, 'start': 265.4, 'end': 265.71999999999997, 'text': ' Right.', 'tokens': [51160, 1779, 13, 51176], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 85, 'seek': 24948, 'start': 265.71999999999997, 'end': 269.32, 'text': \" So let's take automated trading, for example.\", 'tokens': [51176, 407, 718, 311, 747, 18473, 9529, 11, 337, 1365, 13, 51356], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 86, 'seek': 24948, 'start': 269.32, 'end': 272.36, 'text': ' AI agents analyze vast data sets filled with data', 'tokens': [51356, 7318, 12554, 12477, 8369, 1412, 6352, 6412, 365, 1412, 51508], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 87, 'seek': 24948, 'start': 272.36, 'end': 275.0, 'text': ' like historical trends and current news.', 'tokens': [51508, 411, 8584, 13892, 293, 2190, 2583, 13, 51640], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 88, 'seek': 24948, 'start': 275.0, 'end': 278.12, 'text': ' And then they use that information to extract insights.', 'tokens': [51640, 400, 550, 436, 764, 300, 1589, 281, 8947, 14310, 13, 51796], 'temperature': 0.0, 'avg_logprob': -0.22576772516424007, 'compression_ratio': 1.6827309236947792, 'no_speech_prob': 0.0018313084729015827}\n",
      "{'id': 89, 'seek': 27812, 'start': 278.12, 'end': 281.88, 'text': ' And those insights predict how the market will behave and execute trades', 'tokens': [50364, 400, 729, 14310, 6069, 577, 264, 2142, 486, 15158, 293, 14483, 21287, 50552], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 90, 'seek': 27812, 'start': 281.88, 'end': 284.84000000000003, 'text': ' in real time based upon those predictive algorithms.', 'tokens': [50552, 294, 957, 565, 2361, 3564, 729, 35521, 14642, 13, 50700], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 91, 'seek': 27812, 'start': 285.56, 'end': 286.04, 'text': ' Super cool.', 'tokens': [50736, 4548, 1627, 13, 50760], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 92, 'seek': 27812, 'start': 286.68, 'end': 290.2, 'text': ' So assistance, they handle your routine tasks.', 'tokens': [50792, 407, 9683, 11, 436, 4813, 428, 9927, 9608, 13, 50968], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 93, 'seek': 27812, 'start': 290.2, 'end': 294.6, 'text': ' While agents, they take on more complex high-level challenges.', 'tokens': [50968, 3987, 12554, 11, 436, 747, 322, 544, 3997, 1090, 12, 12418, 4759, 13, 51188], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 94, 'seek': 27812, 'start': 294.6, 'end': 299.96, 'text': ' And AI agents, they can scale across multiple tasks all of the same time', 'tokens': [51188, 400, 7318, 12554, 11, 436, 393, 4373, 2108, 3866, 9608, 439, 295, 264, 912, 565, 51456], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 95, 'seek': 27812, 'start': 299.96, 'end': 303.64, 'text': ' without any human intervention, making them ideal for dynamic', 'tokens': [51456, 1553, 604, 1952, 13176, 11, 1455, 552, 7157, 337, 8546, 51640], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 96, 'seek': 27812, 'start': 303.64, 'end': 305.16, 'text': ' and ambiguous problems.', 'tokens': [51640, 293, 39465, 2740, 13, 51716], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 97, 'seek': 27812, 'start': 305.8, 'end': 307.08, 'text': ' Speaking of problems, though.', 'tokens': [51748, 13069, 295, 2740, 11, 1673, 13, 51812], 'temperature': 0.0, 'avg_logprob': -0.16484566490248878, 'compression_ratio': 1.6329588014981273, 'no_speech_prob': 0.0007389502134174109}\n",
      "{'id': 98, 'seek': 30708, 'start': 307.64, 'end': 312.12, 'text': ' AI assistants and AI agents can have limitations', 'tokens': [50392, 7318, 34949, 293, 7318, 12554, 393, 362, 15705, 50616], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 99, 'seek': 30708, 'start': 312.12, 'end': 313.64, 'text': ' like brittleness.', 'tokens': [50616, 411, 738, 593, 45887, 13, 50692], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 100, 'seek': 30708, 'start': 313.64, 'end': 316.84, 'text': ' When slight changes to prompts lead to errors.', 'tokens': [50692, 1133, 4036, 2962, 281, 41095, 1477, 281, 13603, 13, 50852], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 101, 'seek': 30708, 'start': 316.84, 'end': 319.32, 'text': ' Yes, beware the rabbit hole.', 'tokens': [50852, 1079, 11, 312, 3039, 264, 19509, 5458, 13, 50976], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 102, 'seek': 30708, 'start': 319.32, 'end': 322.68, 'text': ' AI agents in particular make it stuck in feedback loops.', 'tokens': [50976, 7318, 12554, 294, 1729, 652, 309, 5541, 294, 5824, 16121, 13, 51144], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 103, 'seek': 30708, 'start': 323.32, 'end': 326.12, 'text': ' Or they might require significant computational resources,', 'tokens': [51176, 1610, 436, 1062, 3651, 4776, 28270, 3593, 11, 51316], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 104, 'seek': 30708, 'start': 326.12, 'end': 327.96, 'text': ' making them expensive to run.', 'tokens': [51316, 1455, 552, 5124, 281, 1190, 13, 51408], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 105, 'seek': 30708, 'start': 327.96, 'end': 333.15999999999997, 'text': ' Without supervision, they can go down all sorts of weird and wonderful paths.', 'tokens': [51408, 9129, 32675, 11, 436, 393, 352, 760, 439, 7527, 295, 3657, 293, 3715, 14518, 13, 51668], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 106, 'seek': 30708, 'start': 334.12, 'end': 336.76, 'text': ' Always good to check those AI outputs.', 'tokens': [51716, 11270, 665, 281, 1520, 729, 7318, 23930, 13, 51848], 'temperature': 0.0, 'avg_logprob': -0.14352038441872111, 'compression_ratio': 1.5576923076923077, 'no_speech_prob': 0.011586526408791542}\n",
      "{'id': 107, 'seek': 33676, 'start': 336.76, 'end': 339.08, 'text': \" But there's improvement happening every day.\", 'tokens': [50364, 583, 456, 311, 10444, 2737, 633, 786, 13, 50480], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 108, 'seek': 33676, 'start': 339.08, 'end': 343.64, 'text': \" And as AI agents improve, we'll likely see them tackling even more complex\", 'tokens': [50480, 400, 382, 7318, 12554, 3470, 11, 321, 603, 3700, 536, 552, 34415, 754, 544, 3997, 50708], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 109, 'seek': 33676, 'start': 343.64, 'end': 346.12, 'text': ' problems without needing human assistance.', 'tokens': [50708, 2740, 1553, 18006, 1952, 9683, 13, 50832], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 110, 'seek': 33676, 'start': 346.12, 'end': 349.15999999999997, 'text': \" And we're already seeing improvements in model reasoning.\", 'tokens': [50832, 400, 321, 434, 1217, 2577, 13797, 294, 2316, 21577, 13, 50984], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 111, 'seek': 33676, 'start': 349.15999999999997, 'end': 353.8, 'text': ' Meaning that AI agents will become more reliable and effective over time.', 'tokens': [50984, 19948, 300, 7318, 12554, 486, 1813, 544, 12924, 293, 4942, 670, 565, 13, 51216], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 112, 'seek': 33676, 'start': 353.8, 'end': 357.48, 'text': \" Take, for example, open AI's O1 model,\", 'tokens': [51216, 3664, 11, 337, 1365, 11, 1269, 7318, 311, 422, 16, 2316, 11, 51400], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 113, 'seek': 33676, 'start': 357.48, 'end': 359.8, 'text': ' which performs reasoning at inference time.', 'tokens': [51400, 597, 26213, 21577, 412, 38253, 565, 13, 51516], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 114, 'seek': 33676, 'start': 360.44, 'end': 365.96, 'text': ' So to recap, AI assistants, they help with your day-to-day tasks.', 'tokens': [51548, 407, 281, 20928, 11, 7318, 34949, 11, 436, 854, 365, 428, 786, 12, 1353, 12, 810, 9608, 13, 51824], 'temperature': 0.0, 'avg_logprob': -0.1438744179556303, 'compression_ratio': 1.6468401486988848, 'no_speech_prob': 0.00028465388459153473}\n",
      "{'id': 115, 'seek': 36596, 'start': 366.03999999999996, 'end': 370.91999999999996, 'text': ' While AI agents take on a more autonomous approach to problem solving.', 'tokens': [50368, 3987, 7318, 12554, 747, 322, 257, 544, 23797, 3109, 281, 1154, 12606, 13, 50612], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 116, 'seek': 36596, 'start': 370.91999999999996, 'end': 373.56, 'text': \" And it's not necessarily an either or here.\", 'tokens': [50612, 400, 309, 311, 406, 4725, 364, 2139, 420, 510, 13, 50744], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 117, 'seek': 36596, 'start': 373.56, 'end': 375.79999999999995, 'text': ' Because as these technologies evolve,', 'tokens': [50744, 1436, 382, 613, 7943, 16693, 11, 50856], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 118, 'seek': 36596, 'start': 375.79999999999995, 'end': 379.79999999999995, 'text': ' expect to see more synergy between assistance and agents', 'tokens': [50856, 2066, 281, 536, 544, 50163, 1296, 9683, 293, 12554, 51056], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 119, 'seek': 36596, 'start': 379.79999999999995, 'end': 384.44, 'text': ' combining their strengths to tackle both simple and complex tasks.', 'tokens': [51056, 21928, 641, 16986, 281, 14896, 1293, 2199, 293, 3997, 9608, 13, 51288], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 120, 'seek': 36596, 'start': 384.44, 'end': 388.2, 'text': \" It's the movie stars, assistant and agent.\", 'tokens': [51288, 467, 311, 264, 3169, 6105, 11, 10994, 293, 9461, 13, 51476], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 121, 'seek': 36596, 'start': 388.2, 'end': 392.35999999999996, 'text': ' Not only working together, but also knowing the best way to do so.', 'tokens': [51476, 1726, 787, 1364, 1214, 11, 457, 611, 5276, 264, 1151, 636, 281, 360, 370, 13, 51684], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 122, 'seek': 36596, 'start': 393.0, 'end': 394.12, 'text': \" Even if you don't.\", 'tokens': [51716, 2754, 498, 291, 500, 380, 13, 51772], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 123, 'seek': 36596, 'start': 394.67999999999995, 'end': 395.15999999999997, 'text': ' Nice.', 'tokens': [51800, 5490, 13, 51824], 'temperature': 0.0, 'avg_logprob': -0.17766074378891747, 'compression_ratio': 1.5807692307692307, 'no_speech_prob': 0.0017407145351171494}\n",
      "{'id': 124, 'seek': 39596, 'start': 395.96, 'end': 397.23999999999995, 'text': ' Oh god.', 'tokens': [50364, 876, 3044, 13, 50428], 'temperature': 1.0, 'avg_logprob': -2.4610865910847983, 'compression_ratio': 0.4666666666666667, 'no_speech_prob': 0.197881281375885}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:52:55.976917Z",
     "start_time": "2024-11-22T20:52:55.974648Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('language'))",
   "id": "5b8ef6bfbc267cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2faebce155146c35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
